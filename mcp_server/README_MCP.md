# MCP Server
Instru√ß√µes de uso ser√£o adicionadas posteriormente.
## üõ†Ô∏è MCP Server - Model Context Protocol

Este diret√≥rio cont√©m a implementa√ß√£o do servidor MCP, que exp√µe funcionalidades de IA (Ferramentas) para Large Language Models (LLMs) externos.

### [cite_start]1. Arquitetura do MCP Server 

A arquitetura segue o padr√£o de **Service Discovery** do MCP:

* **Endpoint de Descoberta (`/mcp.json`):** √â o ponto de entrada. [cite_start]Um cliente LLM (como o ChatGPT) faz uma requisi√ß√£o GET para este endpoint para descobrir quais ferramentas est√£o dispon√≠veis e como cham√°-las (schemas de entrada/sa√≠da, endpoint, m√©todo HTTP).
* **M√≥dulos de Ferramentas (`tools/`):** Cont√©m o c√≥digo Python das ferramentas reais (`summarization_tool.py`).
* **Endpoint de Execu√ß√£o (`/v1/tools/summarize_text`):** √â o endpoint real que o cliente LLM chama (via POST) quando decide usar a ferramenta. Ele recebe o JSON de entrada e retorna o JSON de sa√≠da.
* **Integra√ß√£o com o Core:** A ferramenta `summarize_text` reutiliza o `SummarizationService` principal do projeto (que lida com chunking, paralelismo e altern√¢ncia de modelos).

### 2. Ferramentas Dispon√≠veis

| Nome | Descri√ß√£o | Endpoint de Execu√ß√£o |
| :--- | :--- | :--- |
| `summarize_text`  | Sumariza textos longos usando a estrat√©gia de IA configurada. | `/v1/tools/summarize_text` |

### 3. Instru√ß√µes de Conex√£o do Cliente MCP (Exemplo: ChatGPT/Gemini) 

Para que um cliente LLM (que suporte a integra√ß√£o de ferramentas) utilize este servidor, siga estes passos:

1.  **Garantir o Acesso:** Certifique-se de que este servidor esteja acess√≠vel publicamente (ex: rodando em um dom√≠nio `https://mcp.seuservico.com`). Se estiver rodando localmente, voc√™ pode usar um t√∫nel como `ngrok`.
2.  **Configura√ß√£o no Cliente:** No painel de configura√ß√£o de ferramentas do seu LLM (por exemplo, na se√ß√£o de "Plugins", "Tools" ou "Function Calling"), forne√ßa a URL completa do endpoint de descoberta.

    > **URL de Descoberta:** `http://localhost:8080/mcp.json` (ou o seu endere√ßo p√∫blico)

3.  **Execu√ß√£o:** O LLM ler√° o manifest, entender√° a fun√ß√£o `summarize_text`  e, se o usu√°rio solicitar um resumo de um texto longo, o modelo far√° automaticamente a chamada `POST` para o endpoint `/v1/tools/summarize_text`.

### 4. Logs e Toler√¢ncia a Falhas [cite: 72]

* [cite_start]**Logs B√°sicos:** O servidor utiliza o logger estruturado (JSON) do projeto principal (`app/core/logger.py`) para registrar todas as chamadas e resultados[cite: 72].
* [cite_start]**Tratamento M√≠nimo de Erro:** O *endpoint* de execu√ß√£o da ferramenta possui um bloco `try...except` para capturar erros internos, retornar um HTTP 500 em caso de falha, e registrar o erro, garantindo que o servidor n√£o caia e o modelo cliente receba um erro descritivo[cite: 72].